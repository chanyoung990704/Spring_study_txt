머신 러닝에서의 선형 회귀
	종속 변수와 독립 변수 간 관계를 나타내는 알고리즘.
	독립 변수 간 가중치를 설정해 적절한 종속 변수로 변환이 되게 한다.
	
선형 문제란?
	변수 간의 관계성을 갖는 문제.
		ex) 한 변수의 값이 증가하면 다른 변수의 값도 증가한다.
	선형 관계를 갖는 변수들을 조건으로 갖는 문제.



머신 러닝
	Supervised Learning (지도 학습)
		데이터에 적절한 태그를 설정 (메타 데이터)
		메타 데이터를 통해 모델로 학습
		연속형 변수일 때 Regression(회귀)
		범주형 변수일 때 Classification(분류)
			회귀
				변수들 간 상관관계를 찾는다.
				연속적인 데이터로부터 결과를 예측한다.
				예측 결과가 숫자일 때 사용.

			분류
				주어진 데이터를 정해진 범주에 따라 분류
				예측 결과가 숫자가 아닐 때 사용.
				




	Unsupervised Learning (비지도 학습)
		데이터에 태그 설정하지 않음 ( 사전에 주어지는 정보가 없다 )
		모델이 적절한 패턴을 찾는다.
		




	Reinforcement Learning (강화 학습)
		행동에 보상과 처벌이 존재함.
		여러 번의 행동을 통해 최종적으로 보상을 높게 받을 수 있는 행동을 선택


선형 회귀
	독립 변수 == 입력 변수 (feature)
	종속 변수 == 출력 변수 (targer, label)
	실제 값과 예측 값 차이의 제곱 => MSE loss function (손실함수)
		=> 오차 측정 후 최소화

	실습
		ScikitLearn


-----------------------------------------------------------------------------------
훈련 셋( Train set)
	모델 학습


테스트 셋 ( test set)
	모델 검증	

=> 실습 sklearn.model.selection import train_test_split 을 통해 데이터 셋 분리 가능

-----------------------------------------------------------------------------------

잔차 제곱의 합 : RSS (Residual Sum of Squares)
	= SSR (Sum of Squared Residuals)


최소제곱법 : OLS(Ordinary Least Squares)
	=Least Suqare Method
	노이즈에 취약한 단점


-----------------------------------------------------------------------------------

경사하강법
	오차를 조금씩 줄여나가는 방법
	기울기가 낮아지는 쪽으로
	적절한 Learning Rate를 선택해야 함
	Epoch:
		최적의 값을 찾기위해서 훈련 데이터 셋에 있는 모든 데이터들을 한 번씩 모두 사용하는 방법

확률적 경사 하강법
	기본 경사 하강법의 경우 자원이 많이 소요될 가능성이 존재
	따라서 임의의 데이터를 골라 경사 하강법을 진행하는 방법
	
------------------------------------------------------------------------------------
다중 선형 회귀
	독립 변수들이 여러 개 존재
	One-Hot-Encoding ( 실습 )
		숫자가 아닌 범주형 데이터 존재할 때 사용
		표현하고 싶은 값만 1로, 나머지는 0으로 설정
		해당 범주만큼 column을 추가한다.
		범주가 결과에 영향을 미치지 않는 경우에 사용 (값을 1로)
	다중 공선성
		독립변수들 간 서로 강한 상관관계를 가지면서 회귀계수 추정의 오류가 나타나는 문제
			=> 하나의 feature가 다른 feature에 영향을 끼친다.
		해결법
			Dummy Column이 n 개면 n -1 개만 사용한다. (dummy variable trap)
	
	


------------------------------------------------------------------------------------
평가
	MAE ( Mean Absolute Error )
	: 실제 값과 예측 값 차이의 절대값들의 평균

	MSE ( Mean Squared Error )
	: 실제 값과 예측 값 차이의 제곱한 값들의 평균

	RMSE ( Root Mean Squared Erro )
	: MSE에 루트를 적용
		=> 해석에 유리
	R^2 ( R Square )
	: 결정계수
		=> R^2이 1에 가까우면 좋은 모델, 0이면 나쁜 모델
		=> 실제 값 - 예측 값 / 실제 값 - 평균 값

---------------------------------------------------------------------------------------
다항 회귀
	degree가 많아질 수록 실제 값에 가까운 그래프를 얻을 수 있다.
	=> overfitting 가능성
---------------------------------------------------------------------------------------
로지스틱 회귀 (분류 모델)
	선형 회귀 방식을 분류에 적용한 알고리즘
	데이터가 범주에 속할 확률을 0 ~ 1 사이의 값으로 예측
	더 높은 범주에 속하는 쪽으로 분류 ex) 스펨 메일, 은행 대출, 악성 여부 ...

	시그모이드 함수
		값을 0 ~ 1사이의 값으로 변환
	
---------------------------------------------------------------------------------------
비지도 학습
	정답이 없는 데이터를 통해 유의미한 패턴 / 구조 발견
	Clustering ( 군집화 )
		유사한 특징을 갖는 데이터들을 그룹화 ex) 고객 세분화, 소셜 네트워크 분석, 기사 그룹 분류
		분류와 다르다.
	
		K-Means
			데이터를 K 개의 클러스터(그룹)로 군집화하는 알고리즘
			각 데이터로부터 이들이 속한 클러스터의 중심점까지의 평균 거리를 계산
			동작 순서
				1. K값 설정
				2. 지정된 K개 만큼의 랜덤 좌표 설정
				3. 모든 데이터로부터 가장 가까운 중심점 선택 ( 그룹화 )
				4. 데이터들의 평균 중심으로 중심점 이동
				5. 중심점이 더 이상 이동되지 않을 때까지 반복
			Random Initialization Trap
				랜덤 좌표에 따라 군집화가 무작위 선별

		K-means++
			1. 데이터 중에 랜덤으로 1개의 중심점 선택
			2. 나머지 데이터로부터 중심점까지의 거리 계산
			3. 중심점과 가장 먼 지점의 데이터를 다음 중심점으로 선택
			4. 중심점이 K개가 될 때까지 반복
			5. K-means의 과정 진행


		Optimal K을 선정하기 어려움
		Elbow Method
			1. K의 개수를 늘려가며 중심점까지의 평균 거리 계산
			2. 경사가 완만해지는 지점의 K 선정
		
		Euclidean Distance
			=> 피타고라스 정리 이용
		Manhattan Distance
			=> 각 좌표 차이 절댓값의 합
		Cosine Similarity
			=> 각도가 작다 : 유사하다 / 각도가 크다 : 유사하지 않다

